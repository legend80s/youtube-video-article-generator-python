from sqlalchemy import alias
import os
import json
import uuid
from typing import AsyncIterator, Union

from dotenv import load_dotenv
from langchain_core.messages import AIMessageChunk, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    PromptTemplate,
)
from langchain_core.runnables import Runnable
from langsmith import Client
from pydantic import BaseModel, ConfigDict, Field

from app.lib.models import chatModel
from app.lib.tools.youtube_info import fetch_transcript, YouTubeURL

client = Client()

load_dotenv()

verbose = os.getenv("YAG_VERBOSE") == "True"


def enhance_prompt(
    original_prompt: PromptTemplate, custom_system_prompt: str
) -> ChatPromptTemplate:
    """å¢å¼ºåŸå§‹æç¤ºè¯ï¼Œæ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿæç¤º"""

    system_message = SystemMessage(content=custom_system_prompt)
    # å°†åŸå§‹ PromptTemplate è½¬æ¢ä¸º HumanMessage
    human_message = HumanMessagePromptTemplate.from_template(original_prompt.template)
    # åˆ›å»º ChatPromptTemplate
    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])
    chat_prompt.input_variables = original_prompt.input_variables

    return chat_prompt


# https://smith.langchain.com/hub/muhsinbashir/youtube-transcript-to-article
# muhsinbashir/youtube-transcript-to-articleï¼šConvert any Youtube Video Transcript into an Article ( SEO friendly )
original_prompt: PromptTemplate = client.pull_prompt(
    "muhsinbashir/youtube-transcript-to-article", include_model=True
)


# æ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
custom_system_prompt = """
è¯·æ ¹æ®æä¾›çš„ YouTube è§†é¢‘è½¬å½•æ–‡æœ¬ï¼Œåˆ›ä½œä¸€ç¯‡ Markdown æ ¼å¼æ–‡ç« ã€‚éœ€è¦ï¼š
1. ä¸­æ–‡æ’°å†™
2. ä½¿ç”¨æ°å½“çš„ markdown æ ¼å¼æ ‡é¢˜å±‚çº§(#ã€## ç­‰ï¼Œ# ä¸åº”è¯¥è·Ÿæ ‡é¢˜ã€‚é”™è¯¯ç¤ºä¾‹ï¼šâ€œ# æ ‡é¢˜ï¼šExclusive Or Operationçš„é‡è¦ç‰¹æ€§â€ï¼Œæ­£ç¡®ä¾‹å­ï¼šâ€œ# Exclusive Or Operationçš„é‡è¦ç‰¹æ€§â€)
"""

chat_prompt = enhance_prompt(original_prompt, custom_system_prompt)

prompt = chat_prompt

chain: Runnable = prompt | chatModel


class ItemWithTranscript(BaseModel):
    prompt: str | None = None
    transcript: str
    mode: str | None = None


class Item(BaseModel):
    prompt: str | None = None
    youtube_url: str = Field(description="YouTubeè§†é¢‘URL")
    # youtube_url: str = Field(description="YouTubeè§†é¢‘URL", alias="youtubeUrl")
    mode: str | None = None

    model_config = ConfigDict(
        # å…³é”®é…ç½®
        # populate_by_name=True,  # å…è®¸é€šè¿‡å­—æ®µåæˆ–åˆ«åè®¾ç½®å€¼
        # use_enum_values=True,  # ä½¿ç”¨æšä¸¾å€¼
        # extra="ignore",  # å¿½ç•¥æœªå®šä¹‰çš„å­—æ®µ
        json_schema_extra={  # OpenAPI æ–‡æ¡£ç¤ºä¾‹
            "example": {
                "youtubeUrl": "https://youtube.com/watch?v=abc123",
            }
        },
    )


async def generate(item: Item | ItemWithTranscript) -> str:
    print(f"Received 1 item: {item}")
    if isinstance(item, ItemWithTranscript):
        str_chain: Runnable = chain | StrOutputParser()
        transcript = "\n" + item.transcript

        return await str_chain.ainvoke(input=transcript)

    return "not implemented 1"


async def generate_stream(
    item: Item | ItemWithTranscript,
) -> AsyncIterator[AIMessageChunk]:
    verbose and print(f"[generate_stream] item: {item}")

    # print prompt

    if isinstance(item, ItemWithTranscript):
        verbose and print("[generate_stream] ItemWithTranscript")

        transcript = "\n" + item.transcript
        # print(f"Prompt: {prompt.format(transcript=transcript)}")
        return chain.astream(input=transcript)
    else:
        url: str = item.youtube_url
        verbose and print(f"[generate_stream] only url: {url}")

        # fetch transcript by url
        try:
            transcript = await fetch_transcript(YouTubeURL.of(url))
            return chain.astream(input="\n" + transcript)
        except Exception as exception:
            verbose and print(f"ğŸ’¥ [generate_stream] Exception: {exception}")

            # Return error as async generator
            async def error_generator(error_msg: str):
                yield AIMessageChunk(content=f"Error fetching transcript: {error_msg}")

            return error_generator(str(exception))

    # Final fallback for any unhandled case
    async def not_implemented_generator():
        yield AIMessageChunk(content="not implemented")

    return not_implemented_generator()


async def to_vercel_ai_sdk_generator(item: Union[Item, ItemWithTranscript]):
    """ç”ŸæˆSSEæ ¼å¼çš„æµå¼å“åº”"""
    try:
        # è·å–æµå¼è¾“å‡º
        stream = await generate_stream(item)

        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆé”™è¯¯ä¿¡æ¯ï¼‰ï¼Œç›´æ¥è¿”å›
        if isinstance(stream, str):
            yield f"data: {stream}\n\n"
            return

        # åˆå§‹åŒ–id

        id: str = str(uuid.uuid4())
        yield f"data: {json.dumps({'id': id, 'type': 'text-start'})}\n\n"

        # æµå¼è¾“å‡ºå†…å®¹
        async for chunk in stream:
            if not id:
                assert chunk.id, "chunk.id should not be None"
                id = chunk.id
            # å°†æ¯ä¸ªæ•°æ®å—åŒ…è£…ä¸ºSSEæ ¼å¼
            # è½¬æˆ vercel ai sdk æ ¼å¼ id, type: "text-delta", delta: chunk.content
            chunk = {"id": id, "type": "text-delta", "delta": chunk.content}
            # yield f"data: {chunk}\n\n"
            # json dump
            chunk = json.dumps(chunk)
            # yield f"data: {chunk}\n\n"
            yield f"data: {chunk}\n\n"

        # å‘é€ç»“æŸä¿¡å· id, type: "text-end"
        yield f"data: {json.dumps({'id': id, 'type': 'text-end'})}\n\n"
        yield "data: [DONE]\n\n"

    except Exception as e:
        # é”™è¯¯å¤„ç†
        yield f"data: Error: {str(e)}\n\n"


__all__ = ["generate", "to_vercel_ai_sdk_generator"]
